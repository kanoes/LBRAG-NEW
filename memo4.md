为什么我们可以和需要使用EM和F1来判断回答准确度

# 1. EM（Exact Match）到底在判断什么？

你的理解完全正确：
EM 是非常严格的度量：

> **只要预测答案与标准答案在「规范化后」有任何差别，就判 0 分。**

例如你说的：

* 预测：**4429米**
* 标准：**4429m**

规范化后变成：

* 预测 →** **`4429米`
* 标准 →** **`4429m`

仍然不同 →** ****EM = 0**

---

# ❗ 那 EM 适合测 LLM 吗？

### ➜** ****不适合测 LLM 的语义理解能力。**

### ➜ 它只适合测 “LLM 是否能给出完全匹配的短答案格式”。

例如用于：

* QA 竞赛（SQuAD、HotpotQA 等）
* 事实类简短问答
* 数字类、命名实体类答案（如人名、年份）

LLM 多样性输出会让 EM 非常苛刻。

所以：

### ✔ EM 更像是 “严格格式判定”，不适合衡量真实语义能力。

---

# 🎯 2. F1 Score 是否比 EM 更好？

F1 的处理方式：

> 对预测答案与标准答案进行 token 切分
> → 做 “overlap（交集）”
> → 算 Precision/Recall
> → 合成 F1

代码里：

* 英文 → 单词 token
* 中文/日文 → 字符 token

### 你说的例子：

* 预测：4429米
* 标准：4429m

normalize 后：

* `4429米`
* `4429m`

token 划分：

* 中文是逐字：
  * 预测 tokens = ["4","4","2","9","米"]
  * 标准 tokens = ["4","4","2","9","m"]

overlap = ["4","4","2","9"] → 4 个
precision = 4/5
recall = 4/5
F1 ≈ 0.8

所以：

### ✔ F1 给了“部分匹配”，比 EM 更合理

### ✘ 但它仍然不是语义准确性的完美测量

比如：

预测：

> “这座山大概 4400 米左右吧”

标准：

> “4429 米”

F1 token overlap 大部分相同 → F1 很高
但语义其实是模糊、不精确的。

---

# 🎯 3. 那 EM / F1 为什么仍然被广泛用于 LLM？

因为：

## ✔（1）它们可重复、可自动化

无需人工标注
→ 能测数千 test cases

## ✔（2）它们是事实型 QA 历史基线

你跟 SQuAD、NaturalQuestions、MKQA 全部对齐
→ 强制 LLM 不能胡扯
→ 强制 LLM 不能输出太多解释
→ 强制格式统一

## ✔（3）你的实验场景是“事实类知识问答”，非常适合 EM / F1

你的论文对标：**CrossRAG、XRAG 体系**
这些论文全部用 EM / F1 作为评价指标。

==> 所以用 EM / F1** ****是为了对齐文献标准，而不是因为它们完美。**

---

# 🎯 4. 是什么让 EM / F1 在多语言 QA 里仍然有价值？

因为：

### ❗ 你的实验目标** ****不是测 LLM 的语义能力**

而是测：

* 检索是否找到了正确证据
* 翻译是否保持关键 slot（数字/名称）不丢失
* LBRAG 的多语言桥接是否让回答更接近 ground truth
* 是否减少语言 drift、减少 hallucination

在这些任务里：

### ✔** ****数字（数字、单位）、命名实体匹配是关键**

### ✔ EM / F1 是检验“事实是否精确”的可靠工具

### ✔ F1 特别适合数字类答案（几乎一模一样，但单位不同）

所以它们非常适合用来评价你的系统。
